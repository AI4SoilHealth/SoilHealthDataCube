{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9305d568-6e58-4aea-9467-5eb3341f7d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from eumap.misc import find_files, nan_percentile, GoogleSheet, ttprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GroupKFold\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tool_kit import calc_ccc, accuracy_plot, uncertainty_plot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, HalvingGridSearchCV, KFold, GroupKFold\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# read in necessary material\n",
    "folder = '/mnt/primus/xuemeng_tmp_harbour/soc'\n",
    "cal = pd.read_csv(f'{folder}/data/006.0_cal.pnts_oc.csv',low_memory=False)\n",
    "# cal = pd.read_csv(f'{folder}/data/006.1_cal.pnts_oc.org.csv',low_memory=False)\n",
    "# cal = pd.read_csv(f'{folder}/data/006.2_cal.pnts_oc.mnr.csv',low_memory=False)\n",
    "\n",
    "cal = pd.concat([cal] * 10, ignore_index=True)\n",
    "\n",
    "# covariates\n",
    "# /SOC-EU/features/002_selected.covar_rank.freq.txt\n",
    "with open(f'/mnt/primus/xuemeng_tmp_harbour/soc/SOC-EU/features/002_selected.covar_rank.freq.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "covs = [line.strip() for line in lines]\n",
    "\n",
    "# dataset\n",
    "cal = cal.dropna(subset=covs,how='any')\n",
    "\n",
    "# target variable\n",
    "tgt = 'oc_log1p'\n",
    "# tgt= 'oc'\n",
    "\n",
    "# spatial cross validation\n",
    "spatial_cv_column = 'tile_id'\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# score\n",
    "from tool_kit import calc_ccc\n",
    "from sklearn.metrics import make_scorer\n",
    "ccc_scorer = make_scorer(calc_ccc, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a35eda-9324-41bf-89a5-879e0ce6f64a",
   "metadata": {},
   "source": [
    "### Parameter fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225a24a8-3fd4-44e3-a61a-15772e8c0f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitting_score =  ccc_scorer #'neg_root_mean_squared_error' \n",
    "score_name = 'ccc'\n",
    "space = 'log1p'\n",
    "\n",
    "model_index = 5\n",
    "sample_weights = cal['oc_qa'].values**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb95e2d-9c21-4c55-ac64-df25d5b96070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # random forest\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500, 800, 1000],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': [0.3, 0.5, 0.7, 'log2', 'sqrt'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "tune_rf = HalvingGridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring=fitting_score,\n",
    "    n_jobs=90, \n",
    "    cv=cv,\n",
    "    verbose=1\n",
    ")\n",
    "tune_rf.fit(cal[covs], cal[tgt], sample_weight=sample_weights, groups=cal[spatial_cv_column])\n",
    "if isinstance(sample_weights,int):\n",
    "    weight = ''\n",
    "    ttprint(f'start fine tuning rf{weight}')\n",
    "    tune_rf.fit(cal[covs], cal[tgt], groups=cal[spatial_cv_column])\n",
    "else:\n",
    "    weight = '.weighted'\n",
    "    ttprint(f'start fine tuning rf{weight}')\n",
    "    tune_rf.fit(cal[covs], cal[tgt], sample_weight=sample_weights, groups=cal[spatial_cv_column])\n",
    "ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_rf.best_params_)\n",
    "# joblib.dump(tune_rf.best_params_, f'{folder}/SOC-EU/model/test_best.params_rf.{space}.{score_name}{weight}.joblib')\n",
    "joblib.dump(tune_rf.best_estimator_, f'{folder}/SOC-EU/model/test_model.org.balanced_rf.{space}.{score_name}{weight}.joblib')\n",
    "\n",
    "\n",
    "# # simple ANN\n",
    "# model_index = model_index+1\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import joblib\n",
    "\n",
    "# from sklearn import set_config\n",
    "# set_config(enable_metadata_routing=True)\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('mlp', MLPRegressor(max_iter=5000, early_stopping=True, random_state=42))\n",
    "# ])\n",
    "# pipeline['mlp'].set_score_request(sample_weight=True)\n",
    "\n",
    "# param_grid_ann = {\n",
    "#     'mlp__hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100)],  # NN structure\n",
    "#     'mlp__activation': ['tanh', 'relu'],  # commonly used activation functions in NN\n",
    "#     'mlp__solver': ['adam', 'sgd'],  # optimizer\n",
    "#     'mlp__alpha': [0.0001, 0.001, 0.01],  # regularization to prevent overfitting\n",
    "#     'mlp__learning_rate': ['constant', 'adaptive'],  # how aggressive the weights update\n",
    "#     'mlp__learning_rate_init': [0.001, 0.01]  # initial learning rate\n",
    "    \n",
    "# }\n",
    "\n",
    "# # Define the HalvingGridSearchCV with the pipeline\n",
    "# tune_ann = HalvingGridSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_grid=param_grid_ann,\n",
    "#     scoring=fitting_score,\n",
    "#     n_jobs=-1,\n",
    "#     cv=cv,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# if isinstance(sample_weights,int):\n",
    "#     weight = ''\n",
    "#     ttprint(f'start fine tuning ann{weight}')\n",
    "#     tune_ann.fit(cal[covs], cal[tgt], groups=cal[spatial_cv_column])\n",
    "# else:\n",
    "#     weight = '.weighted'\n",
    "#     ttprint(f'start fine tuning ann{weight}')\n",
    "#     tune_ann.fit(cal[covs], cal[tgt], mlp__sample_weight = sample_weights, groups=cal[spatial_cv_column])\n",
    "# ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_ann.best_params_)\n",
    "\n",
    "# joblib.dump(tune_ann.best_params_, f'{folder}/SOC-EU/model/00{int(model_index)}.0_best.params_ann.log1p.{score_name}{weight}.joblib')\n",
    "# joblib.dump(tune_ann.best_estimator_, f'{folder}/SOC-EU/model/00{int(model_index)}.1_model_ann.log1p.{score_name}{weight}.joblib')\n",
    "\n",
    "\n",
    "# # cubist\n",
    "# # model_index = model_index+1\n",
    "# from cubist import Cubist\n",
    "# # https://pypi.org/project/cubist/\n",
    "# # rule-based predictive model\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# import joblib\n",
    "# from cubist import Cubist\n",
    "# import warnings\n",
    "# tgt='oc'\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('cubist', Cubist())\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Define the parameter grid for Cubist within the pipeline\n",
    "# param_cubist = {\n",
    "#     'cubist__n_rules': [100, 300, 500],  # number of rules to be generated\n",
    "#     'cubist__n_committees': [1, 5, 10],  # committee: ensembles of models\n",
    "#     'cubist__neighbors': [None, 3, 6, 9],  # number of nearest neighbors to use when making a prediction\n",
    "#     'cubist__unbiased': [False, True],  # whether or not to use an unbiased method of rule generation\n",
    "#     'cubist__extrapolation': [0.02, 0.05],  # limits the extent to which predictions can extrapolate beyond the range of the calibration data, a fraction of the total range of the target variable\n",
    "#     'cubist__sample': [None]  # fraction of the calibration data used in building each model, since the calibration dataset could be very small\n",
    "# }\n",
    "\n",
    "\n",
    "# # Define the HalvingGridSearchCV with the pipeline\n",
    "# tune_cubist = HalvingGridSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_grid=param_cubist,\n",
    "#     scoring=fitting_score,\n",
    "#     n_jobs=90,\n",
    "#     cv=cv\n",
    "# )\n",
    "\n",
    "# # Ensure the data retains feature names\n",
    "# X_cal = pd.DataFrame(cal[covs].values, columns=covs)\n",
    "# y_cal = cal[tgt]\n",
    "\n",
    "# # Start fine-tuning process\n",
    "# warnings.filterwarnings('ignore')\n",
    "# if isinstance(sample_weights,int):\n",
    "#     weight = ''\n",
    "#     ttprint(f'start fine tuning cubist{weight}')\n",
    "#     tune_cubist.fit(X_cal, y_cal, groups=cal[spatial_cv_column])\n",
    "# else:\n",
    "#     weight = '.weighted'\n",
    "#     ttprint(f'start fine tuning cubist{weight}')\n",
    "#     fit_params = {'cubist__sample_weight': sample_weights}\n",
    "#     tune_cubist.fit(X_cal, y_cal, **fit_params, groups=cal[spatial_cv_column])\n",
    "    \n",
    "# ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_cubist.best_params_)\n",
    "\n",
    "# # Save the best parameters and model\n",
    "# joblib.dump(tune_cubist.best_params_, f'{folder}/SOC-EU/model/00{int(model_index)}.0_best.params_cubist.log1p.{score_name}{weight}.joblib')\n",
    "# joblib.dump(tune_cubist.best_estimator_, f'{folder}/SOC-EU/model/00{int(model_index)}.1_model_cubist.log1p.{score_name}{weight}.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb63756-e8ab-403c-b76d-c54089250ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:29:43] start training\n",
      "[12:31:18] finish training\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "params = joblib.load('/mnt/primus/xuemeng_tmp_harbour/soc/SOC-EU/model/test_base.model/004.0_best.params_rf.log1p.ccc.weighted.joblib')\n",
    "\n",
    "ttprint('start training')\n",
    "params['n_jobs'] = 90\n",
    "rf_regressor = RandomForestRegressor(**params)\n",
    "rf_regressor.fit(cal[covs], cal[tgt], sample_weight=sample_weights)\n",
    "ttprint('finish training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394b1871-8c5f-4b43-ad0f-7c3ecaf8ee92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['benchmark_rf.weighted.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_regressor, 'benchmark_rf.weighted.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2067a2-4cc9-4887-95bf-2db57f4c60e1",
   "metadata": {},
   "source": [
    "### enxemble machine learning\n",
    "- loop through each possible combination\n",
    "- record the metrics\n",
    "- select the optimal combination of model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d07b6-17dc-4adf-9afb-bd71f9c8f153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/inca/soc_eu_model/SOC-EU/model/002_model_rf.joblib', '/mnt/inca/soc_eu_model/SOC-EU/model/004_model_lasso.joblib', '/mnt/inca/soc_eu_model/SOC-EU/model/006_model_ann.joblib', '/mnt/inca/soc_eu_model/SOC-EU/model/008_model_cubist.joblib']\n",
      "[07:17:21] fitting rf + lasso\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tool_kit import calc_ccc, accuracy_plot, uncertainty_plot\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Load models\n",
    "model_list = find_files(f'{folder}/SOC-EU/model/','0*model*.joblib')\n",
    "model_list = [str(i) for i in model_list]\n",
    "models = [joblib.load(path) for path in model_list]\n",
    "model_names = [\"rf\", \"lasso\", \"ann\", \"cubist\"]\n",
    "print(model_list)\n",
    "\n",
    "# Generate all combinations of models (2, 3, and 4)\n",
    "combinations = []\n",
    "for r in range(2, 5):\n",
    "    combinations.extend(itertools.combinations(zip(models, model_names), r))\n",
    "    \n",
    "# training dataset\n",
    "sampled_train = train.groupby(spatial_cv_column, group_keys=False).apply(lambda x: x.sample(min(len(x), 10))) # 44% data\n",
    "\n",
    "results = []\n",
    "# Loop through each combination of models\n",
    "for combination in combinations:\n",
    "    estimators = [(name, model) for model, name in combination]\n",
    "    combi_name = ''\n",
    "    for _, name in combination:\n",
    "        combi_name = combi_name+' + '+name\n",
    "    combi_name = combi_name[3::]\n",
    "    if 'rf' not in combi_name:\n",
    "        continue\n",
    "    \n",
    "    ttprint(f'fitting {combi_name}')\n",
    "    # Define the Stacking Regressor\n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LinearRegression()\n",
    "    )\n",
    "    \n",
    "    # Fit the stacking regressor\n",
    "#     y_pred = cross_val_predict(stacking_regressor, sampled_train[covs], sampled_train[tgt], cv=cv, groups=sampled_train[spatial_cv_column], n_jobs=90)  \n",
    "    stacking_regressor.fit(sampled_train[covs], sampled_train[tgt])\n",
    "    ttprint('finish fitting')\n",
    "    y_pred = stacking_regressor.predict(test[covs])\n",
    "    r2, rmse, ccc = accuracy_plot(test[tgt], y_pred, combi_name) # visuliazation\n",
    "    error_spatial_plot(test[tgt], y_pred, test['lat'], test['lon'], combi_name)\n",
    "    sorted_plot(test[tgt],y_pred,combi_name)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"Models\": combi_name,\n",
    "        \"R2_CV\": r2,\n",
    "        \"RMSE_CV\": rmse,\n",
    "        \"CCC_CV\": ccc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv(f'{folder}/SOC-EU/model/011_metrics_cv.eml.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb26ca-aec2-40be-ad36-bed8888bab90",
   "metadata": {},
   "source": [
    "### mapie build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84df759-50bc-43c7-9287-a407a1a1889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapie = MapieRegressor(model, method=\"minmax\", cv=5, n_jobs=90) # this cv is to compute the conformal scores, and spatial cross validation\n",
    "mapie.fit(X[covs], X[tgt], groups=X[spatial_cv_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54dd6a5a-a9d7-4139-a5c1-cdb749245e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train = train.groupby(spatial_cv_column, group_keys=False).apply(lambda x: x.sample(frac=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be51dd87-bb70-4166-a8e1-b1f1d17b5d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5481\n",
      "5481\n"
     ]
    }
   ],
   "source": [
    "print(len(sampled_train[spatial_cv_column].unique())) \n",
    "print(len(train[spatial_cv_column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e81c62-0318-4891-b8f5-60e7653556f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a361f4-2415-4f1a-9073-379debe78781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4473296612392309"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_train)/len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3be9de-dfb0-4c73-ae4a-353e8fd56670",
   "metadata": {},
   "source": [
    "### lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a9b14-10f1-43b7-b314-cd4d085ea064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #lasso linear regression\n",
    "\n",
    "# from sklearn.linear_model import Lasso\n",
    "\n",
    "# param_grid_lasso = {\n",
    "#     'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "# }\n",
    "\n",
    "# tune_lasso = HalvingGridSearchCV(\n",
    "#     estimator=Lasso(),\n",
    "#     param_grid=param_grid_lasso,\n",
    "#     scoring=ccc_scorer,\n",
    "#     n_jobs=-1,\n",
    "#     cv=cv,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# ttprint(f'start parameter fine tuning for Lasso, training size: {len(train)}')\n",
    "# tune_lasso.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "# ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_lasso.best_params_)\n",
    "# joblib.dump(tune_lasso.best_params_, f'{folder}/SOC-EU/model/003_best.params_lasso.joblib')\n",
    "# joblib.dump(tune_lasso.best_estimator_, f'{folder}/SOC-EU/model/004_model_lasso.joblib')\n",
    "\n",
    "# print(f'best parames in the initial test: alpha = 0.001, with negative_rmse as score')\n",
    "\n",
    "\n",
    "# # train cubist with rmse in normal space\n",
    "# from cubist import Cubist\n",
    "# # https://pypi.org/project/cubist/\n",
    "# # rule-based predictive model\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "# import joblib\n",
    "# from cubist import Cubist\n",
    "\n",
    "# tgt = 'oc'\n",
    "# warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# # Define a pipeline that includes scalibration and the Cubist model\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('cubist', Cubist())\n",
    "# ])\n",
    "\n",
    "# # Define the parameter grid for Cubist within the pipeline\n",
    "# param_cubist = {\n",
    "#     'cubist__n_rules': [100, 300, 500],  # number of rules to be generated\n",
    "#     'cubist__n_committees': [1, 5, 10],  # committee: ensembles of models\n",
    "#     'cubist__neighbors': [None, 3, 6, 9],  # number of nearest neighbors to use when making a prediction\n",
    "#     'cubist__unbiased': [False, True],  # whether or not to use an unbiased method of rule generation\n",
    "#     'cubist__extrapolation': [0.02, 0.05],  # limits the extent to which predictions can extrapolate beyond the range of the calibration data, a fraction of the total range of the target variable\n",
    "#     'cubist__sample': [None]  # fraction of the calibration data used in building each model\n",
    "# }\n",
    "\n",
    "# # Define the HalvingGridSearchCV with the pipeline\n",
    "# tune_cubist = HalvingGridSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_grid=param_cubist,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=90,\n",
    "#     cv=cv\n",
    "# )\n",
    "\n",
    "# # Ensure the data retains feature names\n",
    "# X_cal = pd.DataFrame(cal[covs].values, columns=covs)\n",
    "# y_cal = cal[tgt]\n",
    "\n",
    "# # Start fine-tuning process\n",
    "# ttprint('start fine tuning cubist')\n",
    "# tune_cubist.fit(X_cal, y_cal, groups=cal[spatial_cv_column])\n",
    "# ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_cubist.best_params_)\n",
    "\n",
    "# # Save the best parameters and model\n",
    "# joblib.dump(tune_cubist.best_params_, f'{folder}/SOC-EU/model/004.0_best.params_cubist.normal.rmse.joblib')\n",
    "# joblib.dump(tune_cubist.best_estimator_, f'{folder}/SOC-EU/model/004.1_model_cubist.normal.rmse.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
